# ğŸš€ Azure End-to-End Data Engineering Pipeline  

![Azure](https://img.shields.io/badge/Azure-Data%20Engineering-blue)  
![Databricks](https://img.shields.io/badge/Databricks-Lakehouse-orange)  
![ETL](https://img.shields.io/badge/ETL-Pipeline-green)  
![PowerBI](https://img.shields.io/badge/PowerBI-Dashboard-yellow)  

---

## ğŸ“Œ Project Overview  
This project demonstrates the design and implementation of a **real-world Azure Data Engineering pipeline**.  
It integrates **Azure SQL Database, Azure Data Factory (ADF), Azure Data Lake, Databricks, Delta Lake, Unity Catalog, and Power BI** under the **Medallion Architecture** (Bronze â Silver â Gold).  

---

## âœ¨ Key Highlights  
- âš¡ **Incremental data ingestion** using **Change Data Capture (CDC)**  
- ğŸ—‚ï¸ **Star schema modeling** (Facts & Dimensions)  
- ğŸ”„ **Slowly Changing Dimensions (SCD Type 1)** handling  
- ğŸ”‘ **Parameterized pipelines** for scalable deployment  
- ğŸ” **Data governance, lineage, and schema enforcement** with Unity Catalog  
- ğŸ“Š **Power BI dashboards** for visualization  

---

## ğŸ—ï¸ Architecture  

<p align="center">
  <img src="https://raw.githubusercontent.com/tkaushik015/Enterprise-Data-Governance-and-Analytics-Framework/main/architecture.png" width="800"/>
</p>  

The pipeline follows a **layered Medallion Architecture**:  

1. **Data Source** â†’ Azure SQL Database  
   - Source tables hosted in Azure SQL DB  
   - Both **initial** and **incremental loads** staged  

2. **Ingestion Layer (Bronze)** â†’ Azure Data Factory  
   - Data extracted into **Azure Data Lake (Bronze)**  
   - Stored in **Parquet format** for efficiency  
   - Incremental ingestion: only new/updated records  

3. **Transformation Layer (Silver)** â†’ Azure Databricks  
   - Data cleaning, joins, and transformations with **PySpark**  
   - Creation of **dimension & fact tables**  
   - Implementation of **SCD Type 1**  

4. **Serving Layer (Gold)** â†’ Delta Lake  
   - Modeled **star schema** for analytics  
   - Optimized storage for reporting  

5. **Governance & Security** â†’ Unity Catalog  
   - Lineage tracking, schema enforcement, access control  

6. **Visualization** â†’ Power BI  
   - Dashboards for KPIs, business insights, and reporting  

---

## ğŸ”„ Data Flow  

ğŸ“¥ **Data Source**: Azure SQL DB  
â¬‡ï¸  
ğŸ“¦ **Bronze**: Raw ingested data via ADF (Parquet)  
â¬‡ï¸  
ğŸ§¹ **Silver**: Cleaned, enriched data in Databricks (SCD, transformations)  
â¬‡ï¸  
â­ **Gold**: Business-ready data (Facts & Dimensions)  
â¬‡ï¸  
ğŸ“Š **Power BI**: Interactive dashboards  

---

## ğŸ›  Data Pipeline Flow (Mermaid Diagram)

```mermaid
flowchart LR
    A[ğŸ“¥ Azure SQL Database] --> B[âš¡ Azure Data Factory (Bronze)]
    B --> C[ğŸ—„ï¸ Azure Data Lake - Bronze Layer]
    C --> D[ğŸ§¹ Azure Databricks - Silver Layer<br/>Cleaning, Joins, Transformations]
    D --> E[â­ Gold Layer - Facts & Dimensions<br/>Star Schema, Optimized Tables]
    E --> F[ğŸ” Unity Catalog<br/>Governance, Schema Enforcement]
    F --> G[ğŸ“Š Power BI Dashboards<br/>Reports & KPIs]


---

### âœ… 2. Commit & Push  
- Just **paste the above snippet into your README.md** file.  
- Commit â†’ Push to GitHub.  

---

### âœ… 3. Result  
On GitHub, it will render a **dynamic diagram**:  

ğŸ“¥ SQL DB â†’ âš¡ ADF â†’ ğŸ—„ï¸ Bronze â†’ ğŸ§¹ Databricks (Silver) â†’ â­ Gold â†’ ğŸ” Unity Catalog â†’ ğŸ“Š Power BI  

---



ğŸš€ Azure End-to-End Data Engineering Pipeline
<p align="center"> <img src="https://img.shields.io/badge/Azure-Data%20Engineering-blue?logo=microsoftazure&logoColor=white" height="30"/> <img src="https://img.shields.io/badge/Databricks-Lakehouse-red?logo=databricks&logoColor=white" height="30"/> <img src="https://img.shields.io/badge/ETL-Pipeline-green?logo=apache-spark&logoColor=white" height="30"/> </p>
ğŸ“Œ Project Overview

This project demonstrates the design and implementation of a real-world Azure Data Engineering pipeline.
It integrates Azure SQL Database, Azure Data Factory (ADF), Azure Data Lake, Databricks, Delta Lake, Unity Catalog, and Power BI under the Medallion Architecture (Bronze â†’ Silver â†’ Gold).

ğŸ‘‰ Key Highlights:

âš¡ Incremental data ingestion using Change Data Capture (CDC)

ğŸ—‚ï¸ Star schema modeling (Facts & Dimensions)

ğŸ”„ Slowly Changing Dimensions (SCD Type 1) handling

ğŸ”‘ Parameterized pipelines for scalable deployment

ğŸ§¾ Data governance, lineage, and schema enforcement with Unity Catalog

ğŸ“Š Power BI dashboards for visualization

ğŸ—ï¸ Architecture
<p align="center"> <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zVnWJtyGOX_kUIDm6ccCFQ.png" width="700"/> </p>
ğŸ”¹ Data Flow Breakdown

Data Source (Azure SQL Database)

Source tables hosted in Azure SQL Database.

Initial + incremental loads staged from sample datasets.

Ingestion Layer (Azure Data Factory â€“ Bronze Layer)

ADF pipelines extract data into Azure Data Lake (Bronze).

Data stored in Parquet format for efficiency.

Implemented incremental ingestion (only new/updated records).

Transformation Layer (Databricks â€“ Silver Layer)

Data cleaned, validated, and structured into Facts & Dimensions.

Applied Star Schema design for analytics.

Managed Slowly Changing Dimensions (SCD Type 1) using upserts.

Serving Layer (Databricks + Delta Lake â€“ Gold Layer)

Final data written to Delta format for:

ACID transactions

Time Travel

Schema evolution

Optimized for BI and downstream analytics.

Governance & Quality (Unity Catalog)

Centralized governance: RBAC, lineage tracking, auditing, and schema validation.

Visualization (Power BI)

Gold layer connected to Power BI.

Created dashboards for insights and KPIs.

âš™ï¸ Tech Stack

Azure â†’ Data Factory, Data Lake, SQL Database

Databricks â†’ PySpark, Delta Lake, Unity Catalog

Modeling â†’ Star Schema, Facts & Dimensions, SCD Type 1

File Formats â†’ CSV â†’ Parquet â†’ Delta

Governance â†’ Unity Catalog (lineage, auditing, RBAC)

BI â†’ Power BI

ğŸ“Š Scenarios Implemented

âœ… Incremental Data Loading (CDC + Stored Procedures)
âœ… Star Schema with Facts & Dimensions
âœ… Slowly Changing Dimensions (SCD Type 1 â€“ Upserts)
âœ… Schema Evolution & Time Travel (Delta Lake)
âœ… Data Governance with Unity Catalog
âœ… Parameterized Pipelines for flexible deployments

ğŸ“Œ How to Run

Clone the repo and provision Azure resources.

Set up Resource Group â†’ Data Lake â†’ SQL Database â†’ ADF â†’ Databricks.

Load source data into Azure SQL Database.

Configure ADF pipelines for ingestion into Bronze.

Use Databricks notebooks to transform into Silver & Gold.

Connect Power BI to Gold for dashboarding.

âœ¨ This project demonstrates the end-to-end design and implementation of a cloud-native data pipeline, covering ingestion, transformation, governance, and visualization.
